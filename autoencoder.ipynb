{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae962e6-82ad-45d4-8a2e-a5280d423428",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TF AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c167742d-b3fd-4f9f-916d-77e3971434a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce0f9df-f559-427d-ba5d-34d9aa350a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "# from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "# import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a5d4ab-188c-4316-b6fe-9c0bfefef836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, top_k_accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, FunctionTransformer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9eb3d80-2656-40b5-9d33-6bcf63eb1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Activation, Input, Embedding, TextVectorization, Reshape, Add, Concatenate, Flatten, Conv1D, Conv1DTranspose, BatchNormalization\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd65af24-b574-4de6-a50c-746b2009048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0e818e-ab0f-4888-9358-fbe327847bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 150)\n",
      "(10, 50)\n",
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "_, seg_df = load_user_data()\n",
    "print(seg_df.shape)\n",
    "y = pd.read_csv('challengeToFill.csv').drop(columns=['Unnamed: 0']).T.reset_index(drop=True).T\n",
    "y_train = y.iloc[:LABELED_USERS, :TRAIN_SEG]\n",
    "y_test_df = y.iloc[:LABELED_USERS, TRAIN_SEG:]\n",
    "train_df = seg_df.iloc[:LABELED_USERS, :TRAIN_SEG]\n",
    "test_df = seg_df.iloc[:LABELED_USERS, TRAIN_SEG:]\n",
    "submission_df = seg_df.iloc[LABELED_USERS:, TRAIN_SEG:]\n",
    "sentences = train_df.to_numpy().flatten().tolist()\n",
    "# embedding_model = create_embeddings(sentences, vector_size=EMBEDDING_DIM, window=5)\n",
    "# print(embedding_model.wv)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9db376-2ecf-40cb-82c3-e80ae819023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_commands = np.unique(seg_df.to_numpy().flatten().tolist()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580ce8cb-5ee5-4ef8-a6fe-be1c23a84aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500,), (1000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_df.to_numpy().flatten()\n",
    "test = test_df.to_numpy().flatten()\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2925fe03-adc0-4ab5-8e66-de6afd876f6b",
   "metadata": {},
   "source": [
    "w2x = {k: idx for idx, k in enumerate(all_commands)}\n",
    "x2w = {idx: k for idx, k in enumerate(all_commands)}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7eb20552-1615-4dec-801e-be6f8db30fae",
   "metadata": {},
   "source": [
    "def seg_to_hist(segs, vocab=all_commands):\n",
    "    count_vec = CountVectorizer(lowercase=False, vocabulary=vocab)\n",
    "    features = count_vec.transform([' '.join(seg) for seg in segs])\n",
    "    return features.toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "262fb7f9-dca7-497e-831e-eea6b3fbe53c",
   "metadata": {},
   "source": [
    "def tfidf_per_user(uid, top_n=100):\n",
    "    documents = []\n",
    "    for i in range(NUM_USERS):\n",
    "        documents.append(' '.join([' '.join(x) for x in seg_df.iloc[i,:TRAIN_SEG]]))\n",
    "    tfidf = TfidfVectorizer(ngram_range=(3,3))\n",
    "    data = tfidf.fit_transform(documents)\n",
    "    tfidf_features = pd.DataFrame.sparse.from_spmatrix(data, columns=tfidf.get_feature_names_out()).sparse.to_dense().T\n",
    "    tfidf_features.columns = [f'user{i}' for i in range(NUM_USERS)]\n",
    "    top_n_user = tfidf_features.nlargest(top_n, f'user{uid}').index.tolist()\n",
    "    return top_n_user"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f2546db-89b2-4625-8027-3cf1812c1730",
   "metadata": {},
   "source": [
    "def get_all_features(top_n=100):\n",
    "    all_features = []\n",
    "    for uid in range(NUM_USERS):\n",
    "        gram1_hist = seg_to_hist(seg_df.iloc[uid, :])\n",
    "        top_n_user = tfidf_per_user(uid, top_n=top_n)\n",
    "        ngram_hist = seg_to_hist(seg_df.iloc[uid, :], vocab=top_n_user)\n",
    "        # print(gram1_hist.shape)\n",
    "        most_common = np.expand_dims(np.array([w2x[stats.mode(x).mode[0]] for x in seg_df.iloc[uid, :]]), axis=-1)\n",
    "        most_common_count = np.expand_dims(np.array([stats.mode(x).count[0] for x in seg_df.iloc[uid, :]]), axis=-1)\n",
    "        num_unique = np.expand_dims(np.array([len(set(x)) for x in seg_df.iloc[uid, :]]), axis=-1)\n",
    "        # ngram_hist = ngram_to_histogram(seg_df.iloc[uid, :])\n",
    "        all_features.append(np.concatenate([gram1_hist, ngram_hist, most_common, num_unique], axis=1))\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c15e676-eb44-427e-ae4e-242f3ac0f5d1",
   "metadata": {},
   "source": [
    "features_per_user_seg = get_all_features(10)\n",
    "features_per_user_seg[1][0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82f938ef-5902-4101-a55f-2161fa3ab3ad",
   "metadata": {},
   "source": [
    "def build_train_data(uid, features_per_user_seg, other_users):\n",
    "    x_train = features_per_user_seg[uid][:TRAIN_SEG]\n",
    "    for i in range(NUM_USERS):\n",
    "        if i != uid:\n",
    "            x_train = np.concatenate([x_train, features_per_user_seg[i][:other_users]])\n",
    "    y_train = np.concatenate([np.zeros(TRAIN_SEG), np.ones(other_users*(NUM_USERS-1))])\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9c177f7-c7ee-47a2-ae6f-6ddbe938b9e2",
   "metadata": {},
   "source": [
    "def build_train_model(uid, features_per_user_seg,other_users=3, top_n=15):\n",
    "    x_train, y_train = build_train_data(uid, features_per_user_seg,other_users)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_test = y_test_df.iloc[uid].to_numpy()\n",
    "    probas = clf.predict_proba(features_per_user_seg[uid][TRAIN_SEG:])[:,1]\n",
    "    preds = [1 if p in sorted(probas)[-top_n:] else 0 for p in probas]\n",
    "    return get_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf06cd38-e8bd-42f6-a4b4-24be808bd7bd",
   "metadata": {},
   "source": [
    "def build_test_model(uid, features_per_user_seg,other_users=3, top_n=15):\n",
    "    x_train, y_train = build_train_data(uid, features_per_user_seg,other_users)\n",
    "    clf = MLPClassifier(learning_rate='adaptive', hidden_layer_sizes=(100,), max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    probas = clf.predict_proba(features_per_user_seg[uid][TRAIN_SEG:])[:,1]\n",
    "    preds = [1 if p in sorted(probas)[-top_n:] else 0 for p in probas]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fb93af7-34b4-460f-ba3a-4a7b59e32e4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "total_score = 0\n",
    "for uid in range(LABELED_USERS):\n",
    "    score = build_train_model(uid,features_per_user_seg, other_users=2, top_n=20)\n",
    "    total_score += score\n",
    "print(total_score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de76c07-3105-4906-a6ca-a7932eb5adf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "preds = []\n",
    "for uid in range(LABELED_USERS, NUM_USERS):\n",
    "    preds += [build_test_model(uid,features_per_user_seg, 3)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8164c6a4-2497-4610-911e-5ae7f51e75e2",
   "metadata": {},
   "source": [
    "np.array(preds).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed854efc-be2c-436b-b998-2905c61fef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd40e59e-886c-44c5-ae55-f65a9bbf7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(train_df.to_numpy().flatten()).apply(lambda x: ' '.join(x)).values.reshape(-1, 1).tolist()\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(s)\n",
    "vectorize_layer = TextVectorization(output_mode='int', output_sequence_length=SEG_SIZE)\n",
    "vectorize_layer.adapt(text_dataset.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dd8ab613-cf5d-4a00-be3f-330348649808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "    return Sequential([ BatchNormalization(),\n",
    "                        Dense(EMBEDDING_DIM*4, activation='relu'),\n",
    "                        Conv1D(64, 5, activation='relu'),\n",
    "                        Dense(EMBEDDING_DIM*2, activation='relu'),\n",
    "                        Conv1D(32, 3, activation='relu'),\n",
    "                        Dense(EMBEDDING_DIM)\n",
    "    ], name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54c1fc4d-3ddd-4f7f-b0e7-03121cda7e10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv1d is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m build_encoder()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m encoder\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\.conda\\envs\\dl\\lib\\site-packages\\keras\\engine\\sequential.py:345\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    343\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(input_shape)\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_input_shape \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\dl\\lib\\site-packages\\keras\\engine\\training.py:413\u001b[0m, in \u001b[0;36mModel.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    410\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou can only call `build` on a model if its `call` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    411\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod accepts an `inputs` argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    415\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    416\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    417\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstead, in order to instantiate and build your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    418\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel, `call` your model on real tensor data (of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    419\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe correct dtype).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\dl\\lib\\site-packages\\keras\\engine\\sequential.py:383\u001b[0m, in \u001b[0;36mSequential.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m argspec:\n\u001b[0;32m    381\u001b[0m   kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 383\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layer(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    386\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32m~\\.conda\\envs\\dl\\lib\\site-packages\\keras\\engine\\base_layer.py:1020\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1013\u001b[0m eager \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1015\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1016\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   1017\u001b[0m     build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m eager,\n\u001b[0;32m   1018\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining_mode):\n\u001b[1;32m-> 1020\u001b[0m   \u001b[43minput_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_input_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m eager:\n\u001b[0;32m   1022\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32m~\\.conda\\envs\\dl\\lib\\site-packages\\keras\\engine\\input_spec.py:229\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    227\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    228\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(input_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    230\u001b[0m                      layer_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    231\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: expected min_ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(spec\u001b[38;5;241m.\u001b[39mmin_ndim) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    232\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ndim) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    233\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Full shape received: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    234\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(shape)))\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv1d is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 512)"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder()\n",
    "encoder.build(input_shape=(None, SEG_SIZE))\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "845eda19-9321-4479-8a94-8617a16fa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder():\n",
    "    return Sequential([ Conv1DTranspose(32, 3, activation='relu'),\n",
    "                        Dense(EMBEDDING_DIM*2, activation='relu'),\n",
    "                        Conv1DTranspose(64, 5, activation='relu'),\n",
    "                        Dense(EMBEDDING_DIM*4, activation='relu'),\n",
    "                        BatchNormalization()\n",
    "                        Dense(SEG_SIZE)\n",
    "    ], name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2aa697ed-ee73-472b-9286-62b8ee1085f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 215,908\n",
      "Trainable params: 215,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = build_decoder()\n",
    "decoder.build(input_shape=(None, EMBEDDING_DIM))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "71431ff3-2ff8-4351-a624-89bcf3e3463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         (None, 128)               216336    \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         (None, 100)               215908    \n",
      "=================================================================\n",
      "Total params: 432,244\n",
      "Trainable params: 432,044\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Sequential(name='autoencoder')\n",
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)\n",
    "autoencoder.build(input_shape=encoder.input_shape)\n",
    "autoencoder.compile(loss=root_mean_squared_error, optimizer='adam', metrics=RootMeanSquaredError(name='rmse'))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d2eaea6-706d-44ff-9d7d-05f39cb7753e",
   "metadata": {},
   "source": [
    "def build_autoencoder():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(SEG_SIZE)))\n",
    "    model.add(Embedding(vectorize_layer.vocabulary_size(), EMBEDDING_DIM))\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c171421-9050-4bbd-b539-1e9a7ac752c6",
   "metadata": {},
   "source": [
    "autoencoder = build_autoencoder()\n",
    "autoencoder.compile(loss='mae', optimizer='adam', metrics='mae')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0df33c6-4749-4e9a-b7fd-f0e2c3529bcb",
   "metadata": {},
   "source": [
    "train = vectorize_layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6c024b75-d7d4-469b-b5c5-684a8d8ce842",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('model.h5', save_best_only=True, verbose=1, save_weights_only=True)\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=100, mode=\"min\", verbose=1)\n",
    "rlop = ReduceLROnPlateau(patience=30, verbose=2)\n",
    "callbacks = [cp, es, rlop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5c512524-2219-4ea0-91d0-34ec32460852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([500, 100])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21f90ffe-fa87-49cc-80ef-ee9687918874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.,  19.,  19., ..., 116.,   2.,   2.],\n",
       "       [116.,   5.,   5., ...,  15.,  13.,  21.],\n",
       "       [ 21.,  72.,  15., ..., 187.,  21.,  21.],\n",
       "       ...,\n",
       "       [ 52.,  52.,  56., ...,  72.,  15.,  15.],\n",
       "       [ 13.,   5.,  76., ...,  79.,  12., 119.],\n",
       "       [130.,   2.,  18., ...,  18.,  79.,  12.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_1 = train[:TRAIN_SEG, :].numpy().astype(float)\n",
    "user_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "956db824-00ab-47ce-9e59-9e00645dc595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "40/40 [==============================] - 1s 9ms/step - loss: 55.0726 - rmse: 57.9179 - val_loss: 63.5718 - val_rmse: 65.1263\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 63.57179, saving model to model.h5\n",
      "Epoch 2/500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 55.0549 - rmse: 57.9014 - val_loss: 63.8619 - val_rmse: 65.4396\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 63.57179\n",
      "Epoch 3/500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 55.0371 - rmse: 57.8844 - val_loss: 64.3884 - val_rmse: 66.0012\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 63.57179\n",
      "Epoch 4/500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 55.0193 - rmse: 57.8671 - val_loss: 65.2088 - val_rmse: 66.8751\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 63.57179\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 55.0093 - rmse: 57.8581 - val_loss: 66.4524 - val_rmse: 68.1911\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 63.57179\n",
      "Epoch 6/500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 55.0075 - rmse: 57.8564 - val_loss: 68.2341 - val_rmse: 70.0802\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 63.57179\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(user_1, user_1, batch_size=1, epochs=500, validation_split=0.2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4c5a43ce-e7fa-472d-9d3d-8adda856e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model by subclassing Model class in tensorflow\n",
    "class AutoEncoder(Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_units: int\n",
    "    Number of output units\n",
    "\n",
    "    code_size: int\n",
    "    Number of units in bottle neck\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_units, hidden_dim=32, code_size=8):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential([\n",
    "            # Embedding(vectorize_layer.vocabulary_size(), EMBEDDING_DIM),\n",
    "            # LSTM(hidden_dim, return_sequences=True, dropout=0.5),\n",
    "            Dense(hidden_dim, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(hidden_dim//2, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(hidden_dim//4, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            # Dropout(0.1),\n",
    "            # Dense(16, activation='relu'),\n",
    "            # Dropout(0.1),\n",
    "            # Dense(code_size, activation='relu')\n",
    "            ], name='encoder')\n",
    "        \n",
    "        self.decoder = Sequential([\n",
    "            Dense(hidden_dim//4, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(hidden_dim//2, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(hidden_dim, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(output_units, activation='relu'),\n",
    "            Flatten()\n",
    "            ], name='decoder')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "86504eae-ecd0-4808-bef2-6526456ec75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Sequential)         (None, 8)                 1752      \n",
      "_________________________________________________________________\n",
      "decoder (Sequential)         (None, 100)               4060      \n",
      "=================================================================\n",
      "Total params: 5,812\n",
      "Trainable params: 5,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(output_units=SEG_SIZE, hidden_dim=32)\n",
    "# configurations of model\n",
    "model.compile(loss=root_mean_squared_error, metrics=RootMeanSquaredError(name='rmse'), optimizer='adam')\n",
    "model.build(input_shape=(None, SEG_SIZE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee6b90ef-3048-4a5a-b759-15188991b20d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [133]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "train = train.numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c3545221-2db7-4ad6-b594-8fafa3ee9fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 47.7730 - rmse: 49.0299 - val_loss: 51.1093 - val_rmse: 52.3817\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 43.42315\n",
      "Epoch 2/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 46.3056 - rmse: 47.6796 - val_loss: 50.5032 - val_rmse: 51.7649\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 43.42315\n",
      "Epoch 3/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 45.2185 - rmse: 46.4846 - val_loss: 48.4695 - val_rmse: 49.6568\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 43.42315\n",
      "Epoch 4/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 43.9787 - rmse: 44.9779 - val_loss: 48.3367 - val_rmse: 49.5248\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 43.42315\n",
      "Epoch 5/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 43.2523 - rmse: 44.3790 - val_loss: 47.7297 - val_rmse: 48.9148\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 43.42315\n",
      "Epoch 6/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 42.3331 - rmse: 43.5465 - val_loss: 46.6138 - val_rmse: 47.7317\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 43.42315\n",
      "Epoch 7/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 41.8866 - rmse: 43.0213 - val_loss: 46.9419 - val_rmse: 48.0839\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 43.42315\n",
      "Epoch 8/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 41.5021 - rmse: 42.6535 - val_loss: 47.8522 - val_rmse: 49.0512\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 43.42315\n",
      "Epoch 9/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 40.7671 - rmse: 41.8538 - val_loss: 47.1925 - val_rmse: 48.3611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 43.42315\n",
      "Epoch 10/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 40.8511 - rmse: 42.2133 - val_loss: 48.0214 - val_rmse: 49.2368\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 43.42315\n",
      "Epoch 11/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 41.0727 - rmse: 42.0646 - val_loss: 47.4449 - val_rmse: 48.6325\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 43.42315\n",
      "Epoch 12/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 40.1874 - rmse: 41.4492 - val_loss: 47.7987 - val_rmse: 49.0039\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 43.42315\n",
      "Epoch 13/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 40.2528 - rmse: 41.4556 - val_loss: 47.3941 - val_rmse: 48.5684\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 43.42315\n",
      "Epoch 14/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 40.1586 - rmse: 41.2114 - val_loss: 47.0877 - val_rmse: 48.2711\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 43.42315\n",
      "Epoch 15/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 40.2281 - rmse: 41.4418 - val_loss: 47.7573 - val_rmse: 48.9607\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 43.42315\n",
      "Epoch 16/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 40.3215 - rmse: 41.7818 - val_loss: 47.0410 - val_rmse: 48.2216\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 43.42315\n",
      "Epoch 17/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.6112 - rmse: 40.7640 - val_loss: 46.8069 - val_rmse: 47.9684\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 43.42315\n",
      "Epoch 18/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.9765 - rmse: 41.1125 - val_loss: 47.4441 - val_rmse: 48.6449\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 43.42315\n",
      "Epoch 19/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.3859 - rmse: 40.5891 - val_loss: 47.5326 - val_rmse: 48.7426\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 43.42315\n",
      "Epoch 20/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.7236 - rmse: 40.5779 - val_loss: 46.9432 - val_rmse: 48.1131\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 43.42315\n",
      "Epoch 21/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.5342 - rmse: 40.6218 - val_loss: 47.6793 - val_rmse: 48.8904\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 43.42315\n",
      "Epoch 22/500\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 39.8795 - rmse: 40.9988 - val_loss: 48.0051 - val_rmse: 49.2436\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 43.42315\n",
      "Epoch 23/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.3986 - rmse: 40.3549 - val_loss: 46.6748 - val_rmse: 47.8607\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 43.42315\n",
      "Epoch 24/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.4431 - rmse: 40.4180 - val_loss: 47.1637 - val_rmse: 48.3676\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 43.42315\n",
      "Epoch 25/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.6403 - rmse: 40.6591 - val_loss: 47.4536 - val_rmse: 48.6732\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 43.42315\n",
      "Epoch 26/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.8172 - rmse: 40.8064 - val_loss: 47.6951 - val_rmse: 48.9287\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 43.42315\n",
      "Epoch 27/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.4808 - rmse: 40.7653 - val_loss: 47.4028 - val_rmse: 48.6263\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 43.42315\n",
      "Epoch 28/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.2838 - rmse: 40.3123 - val_loss: 48.0323 - val_rmse: 49.2791\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 43.42315\n",
      "Epoch 29/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.5409 - rmse: 40.5912 - val_loss: 47.4471 - val_rmse: 48.6744\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 43.42315\n",
      "Epoch 30/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.3684 - rmse: 40.4899 - val_loss: 47.0879 - val_rmse: 48.3048\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 43.42315\n",
      "Epoch 31/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.8686 - rmse: 40.7426 - val_loss: 47.6332 - val_rmse: 48.8764\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 43.42315\n",
      "Epoch 32/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.8712 - rmse: 40.3891 - val_loss: 47.2409 - val_rmse: 48.4560\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 43.42315\n",
      "Epoch 33/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.2630 - rmse: 40.3756 - val_loss: 47.2125 - val_rmse: 48.4296\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 43.42315\n",
      "Epoch 34/500\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 38.7357 - rmse: 40.0513 - val_loss: 46.7914 - val_rmse: 47.9899\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 43.42315\n",
      "Epoch 35/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.2337 - rmse: 40.2156 - val_loss: 46.8624 - val_rmse: 48.0646\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 43.42315\n",
      "Epoch 36/500\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 39.2041 - rmse: 40.0483 - val_loss: 46.2141 - val_rmse: 47.3944\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 43.42315\n",
      "Epoch 37/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.0451 - rmse: 40.0744 - val_loss: 46.9379 - val_rmse: 48.1510\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 43.42315\n",
      "Epoch 38/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.1474 - rmse: 40.1427 - val_loss: 47.1007 - val_rmse: 48.3150\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 43.42315\n",
      "Epoch 39/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.1538 - rmse: 40.2478 - val_loss: 47.6468 - val_rmse: 48.8910\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 43.42315\n",
      "Epoch 40/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.1555 - rmse: 40.0520 - val_loss: 46.9551 - val_rmse: 48.1678\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 43.42315\n",
      "Epoch 41/500\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 39.0962 - rmse: 40.2156 - val_loss: 46.3309 - val_rmse: 47.5202\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 43.42315\n",
      "Epoch 42/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.2585 - rmse: 40.3528 - val_loss: 47.6819 - val_rmse: 48.9306\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 43.42315\n",
      "Epoch 43/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.0057 - rmse: 40.0071 - val_loss: 47.4080 - val_rmse: 48.6523\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 43.42315\n",
      "Epoch 44/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.2333 - rmse: 40.3408 - val_loss: 47.3163 - val_rmse: 48.5519\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 43.42315\n",
      "Epoch 45/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.9463 - rmse: 40.2217 - val_loss: 48.1182 - val_rmse: 49.3949\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 43.42315\n",
      "Epoch 46/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 39.1260 - rmse: 40.1385 - val_loss: 47.7071 - val_rmse: 48.9572\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 43.42315\n",
      "Epoch 47/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.7469 - rmse: 39.8803 - val_loss: 46.8054 - val_rmse: 48.0257\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 43.42315\n",
      "Epoch 48/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6432 - rmse: 39.8326 - val_loss: 47.2258 - val_rmse: 48.4679\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 43.42315\n",
      "Epoch 49/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4656 - rmse: 39.6271 - val_loss: 46.6809 - val_rmse: 47.8978\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 43.42315\n",
      "Epoch 50/500\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 39.0196 - rmse: 39.9727 - val_loss: 47.0321 - val_rmse: 48.2674\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 43.42315\n",
      "Epoch 51/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5554 - rmse: 39.7044 - val_loss: 47.3782 - val_rmse: 48.6328\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 43.42315\n",
      "Epoch 52/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7516 - rmse: 39.8717 - val_loss: 47.2399 - val_rmse: 48.4974\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 43.42315\n",
      "Epoch 53/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.7532 - rmse: 39.7543 - val_loss: 46.8265 - val_rmse: 48.0751\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 43.42315\n",
      "Epoch 54/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7653 - rmse: 39.7198 - val_loss: 47.1223 - val_rmse: 48.3807\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 43.42315\n",
      "Epoch 55/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5450 - rmse: 39.6042 - val_loss: 46.4579 - val_rmse: 47.6839\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 43.42315\n",
      "Epoch 56/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7818 - rmse: 39.9599 - val_loss: 46.7625 - val_rmse: 47.9913\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 43.42315\n",
      "Epoch 57/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.6744 - rmse: 39.7912 - val_loss: 46.2238 - val_rmse: 47.4368\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 43.42315\n",
      "Epoch 58/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.6070 - rmse: 39.7361 - val_loss: 46.2683 - val_rmse: 47.4896\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 43.42315\n",
      "Epoch 59/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6218 - rmse: 39.8051 - val_loss: 46.8259 - val_rmse: 48.0706\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 43.42315\n",
      "Epoch 60/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6783 - rmse: 39.7705 - val_loss: 46.6774 - val_rmse: 47.9181\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 43.42315\n",
      "Epoch 61/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.9253 - rmse: 39.8702 - val_loss: 46.4331 - val_rmse: 47.6610\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 43.42315\n",
      "Epoch 62/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6747 - rmse: 39.7236 - val_loss: 45.3428 - val_rmse: 46.5341\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 43.42315\n",
      "Epoch 63/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.7161 - rmse: 39.6536 - val_loss: 46.8732 - val_rmse: 48.1236\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 43.42315\n",
      "Epoch 64/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.7443 - rmse: 39.6906 - val_loss: 47.0714 - val_rmse: 48.3314\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 43.42315\n",
      "Epoch 65/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.7904 - rmse: 39.7926 - val_loss: 47.1173 - val_rmse: 48.3825\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 43.42315\n",
      "Epoch 66/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.5593 - rmse: 39.6439 - val_loss: 47.6960 - val_rmse: 48.9796\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 43.42315\n",
      "Epoch 67/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.3963 - rmse: 39.5653 - val_loss: 45.9751 - val_rmse: 47.1905\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 43.42315\n",
      "Epoch 68/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5773 - rmse: 39.4733 - val_loss: 46.9420 - val_rmse: 48.1929\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 43.42315\n",
      "Epoch 69/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.8021 - rmse: 39.9486 - val_loss: 45.9587 - val_rmse: 47.1808\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 43.42315\n",
      "Epoch 70/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.5009 - rmse: 39.5245 - val_loss: 47.0992 - val_rmse: 48.3761\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 43.42315\n",
      "Epoch 71/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.9208 - rmse: 39.9754 - val_loss: 46.2915 - val_rmse: 47.5377\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 43.42315\n",
      "Epoch 72/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5669 - rmse: 39.7469 - val_loss: 46.6183 - val_rmse: 47.8807\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 43.42315\n",
      "Epoch 73/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5487 - rmse: 39.7617 - val_loss: 46.9884 - val_rmse: 48.2685\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 43.42315\n",
      "Epoch 74/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5792 - rmse: 39.6402 - val_loss: 46.6110 - val_rmse: 47.8738\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 43.42315\n",
      "Epoch 75/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3319 - rmse: 39.5425 - val_loss: 46.4247 - val_rmse: 47.6823\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 43.42315\n",
      "Epoch 76/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4818 - rmse: 39.7698 - val_loss: 45.7295 - val_rmse: 46.9557\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 43.42315\n",
      "Epoch 77/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.4088 - rmse: 39.6012 - val_loss: 46.7956 - val_rmse: 48.0686\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 43.42315\n",
      "Epoch 78/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.6888 - rmse: 39.6625 - val_loss: 45.5385 - val_rmse: 46.7536\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 43.42315\n",
      "Epoch 79/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2833 - rmse: 39.2829 - val_loss: 46.4641 - val_rmse: 47.7139\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 43.42315\n",
      "Epoch 80/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5139 - rmse: 39.4970 - val_loss: 46.7115 - val_rmse: 47.9811\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 43.42315\n",
      "Epoch 81/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4018 - rmse: 39.5576 - val_loss: 46.4983 - val_rmse: 47.7528\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 43.42315\n",
      "Epoch 82/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3520 - rmse: 39.4319 - val_loss: 46.0511 - val_rmse: 47.2931\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 43.42315\n",
      "Epoch 83/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4571 - rmse: 39.4755 - val_loss: 46.5459 - val_rmse: 47.8039\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 43.42315\n",
      "Epoch 84/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7374 - rmse: 39.6226 - val_loss: 45.9029 - val_rmse: 47.1384\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 43.42315\n",
      "Epoch 85/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3888 - rmse: 39.4818 - val_loss: 46.7888 - val_rmse: 48.0671\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 43.42315\n",
      "Epoch 86/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4719 - rmse: 39.8942 - val_loss: 46.2192 - val_rmse: 47.4677\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 43.42315\n",
      "Epoch 87/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2514 - rmse: 39.3477 - val_loss: 45.8812 - val_rmse: 47.1255\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 43.42315\n",
      "Epoch 88/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.6528 - rmse: 39.6936 - val_loss: 46.5077 - val_rmse: 47.7816\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 43.42315\n",
      "Epoch 89/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3209 - rmse: 39.3815 - val_loss: 46.3588 - val_rmse: 47.6197\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 43.42315\n",
      "Epoch 90/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3366 - rmse: 39.3366 - val_loss: 45.8211 - val_rmse: 47.0607\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 43.42315\n",
      "Epoch 91/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5073 - rmse: 39.5229 - val_loss: 46.8531 - val_rmse: 48.1456\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 43.42315\n",
      "Epoch 92/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5057 - rmse: 39.5202 - val_loss: 45.8578 - val_rmse: 47.1069\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 43.42315\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 93/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2116 - rmse: 39.2542 - val_loss: 45.7730 - val_rmse: 47.0188\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 43.42315\n",
      "Epoch 94/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6774 - rmse: 39.7391 - val_loss: 46.1411 - val_rmse: 47.4029\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 43.42315\n",
      "Epoch 95/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3921 - rmse: 39.6135 - val_loss: 46.1709 - val_rmse: 47.4352\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 43.42315\n",
      "Epoch 96/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4750 - rmse: 39.5604 - val_loss: 46.0793 - val_rmse: 47.3392\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 43.42315\n",
      "Epoch 97/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3948 - rmse: 39.3781 - val_loss: 46.0224 - val_rmse: 47.2797\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 43.42315\n",
      "Epoch 98/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.0574 - rmse: 39.1333 - val_loss: 46.1553 - val_rmse: 47.4183\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 43.42315\n",
      "Epoch 99/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4913 - rmse: 39.4303 - val_loss: 46.3840 - val_rmse: 47.6562\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 43.42315\n",
      "Epoch 100/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7531 - rmse: 39.6958 - val_loss: 46.4171 - val_rmse: 47.6905\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 43.42315\n",
      "Epoch 101/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4502 - rmse: 39.5468 - val_loss: 46.0219 - val_rmse: 47.2793\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 43.42315\n",
      "Epoch 102/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5558 - rmse: 39.7184 - val_loss: 46.0338 - val_rmse: 47.2926\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 43.42315\n",
      "Epoch 103/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.0912 - rmse: 39.1958 - val_loss: 46.0069 - val_rmse: 47.2638\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 43.42315\n",
      "Epoch 104/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1738 - rmse: 39.1829 - val_loss: 46.1060 - val_rmse: 47.3662\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 43.42315\n",
      "Epoch 105/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6625 - rmse: 39.6719 - val_loss: 46.1286 - val_rmse: 47.3899\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 43.42315\n",
      "Epoch 106/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4328 - rmse: 39.5835 - val_loss: 46.1669 - val_rmse: 47.4301\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 43.42315\n",
      "Epoch 107/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2678 - rmse: 39.3640 - val_loss: 46.2868 - val_rmse: 47.5541\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 43.42315\n",
      "Epoch 108/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4160 - rmse: 39.3026 - val_loss: 46.3202 - val_rmse: 47.5912\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 43.42315\n",
      "Epoch 109/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4500 - rmse: 39.5459 - val_loss: 46.2487 - val_rmse: 47.5176\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 43.42315\n",
      "Epoch 110/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5782 - rmse: 39.6290 - val_loss: 46.1672 - val_rmse: 47.4320\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 43.42315\n",
      "Epoch 111/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4268 - rmse: 39.5683 - val_loss: 46.3604 - val_rmse: 47.6344\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 43.42315\n",
      "Epoch 112/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4715 - rmse: 39.8111 - val_loss: 46.2305 - val_rmse: 47.5001\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 43.42315\n",
      "Epoch 113/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4160 - rmse: 39.5102 - val_loss: 46.4748 - val_rmse: 47.7546\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 43.42315\n",
      "Epoch 114/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6310 - rmse: 39.6700 - val_loss: 46.3785 - val_rmse: 47.6529\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 43.42315\n",
      "Epoch 115/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6015 - rmse: 39.5175 - val_loss: 46.4053 - val_rmse: 47.6807\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 43.42315\n",
      "Epoch 116/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3213 - rmse: 39.4665 - val_loss: 46.2486 - val_rmse: 47.5163\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 43.42315\n",
      "Epoch 117/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2493 - rmse: 39.3806 - val_loss: 46.3457 - val_rmse: 47.6150\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 43.42315\n",
      "Epoch 118/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4221 - rmse: 39.4800 - val_loss: 46.0845 - val_rmse: 47.3439\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 43.42315\n",
      "Epoch 119/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2240 - rmse: 39.3061 - val_loss: 46.0106 - val_rmse: 47.2673\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 43.42315\n",
      "Epoch 120/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 37.9733 - rmse: 39.2961 - val_loss: 46.0123 - val_rmse: 47.2704\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 43.42315\n",
      "Epoch 121/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2435 - rmse: 39.4703 - val_loss: 46.3601 - val_rmse: 47.6333\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 43.42315\n",
      "Epoch 122/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2620 - rmse: 39.2747 - val_loss: 46.1709 - val_rmse: 47.4355\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 43.42315\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 123/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5419 - rmse: 39.5371 - val_loss: 46.2046 - val_rmse: 47.4706\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 43.42315\n",
      "Epoch 124/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4643 - rmse: 39.5979 - val_loss: 46.2115 - val_rmse: 47.4778\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 43.42315\n",
      "Epoch 125/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5506 - rmse: 39.5912 - val_loss: 46.2308 - val_rmse: 47.4977\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 43.42315\n",
      "Epoch 126/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2427 - rmse: 39.2587 - val_loss: 46.2047 - val_rmse: 47.4705\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 43.42315\n",
      "Epoch 127/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1920 - rmse: 39.3011 - val_loss: 46.2283 - val_rmse: 47.4951\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 43.42315\n",
      "Epoch 128/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1499 - rmse: 39.2354 - val_loss: 46.2132 - val_rmse: 47.4794\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 43.42315\n",
      "Epoch 129/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4719 - rmse: 39.5631 - val_loss: 46.2203 - val_rmse: 47.4866\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 43.42315\n",
      "Epoch 130/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2664 - rmse: 39.3591 - val_loss: 46.2230 - val_rmse: 47.4895\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 43.42315\n",
      "Epoch 131/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6416 - rmse: 39.6177 - val_loss: 46.2235 - val_rmse: 47.4900\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 43.42315\n",
      "Epoch 132/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2875 - rmse: 39.3215 - val_loss: 46.2445 - val_rmse: 47.5118\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 43.42315\n",
      "Epoch 133/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1954 - rmse: 39.3567 - val_loss: 46.2408 - val_rmse: 47.5081\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 43.42315\n",
      "Epoch 134/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.0724 - rmse: 39.3352 - val_loss: 46.2593 - val_rmse: 47.5275\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 43.42315\n",
      "Epoch 135/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.2432 - rmse: 39.1857 - val_loss: 46.2669 - val_rmse: 47.5354\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 43.42315\n",
      "Epoch 136/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5601 - rmse: 39.7385 - val_loss: 46.2452 - val_rmse: 47.5129\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 43.42315\n",
      "Epoch 137/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3596 - rmse: 39.5218 - val_loss: 46.2412 - val_rmse: 47.5088\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 43.42315\n",
      "Epoch 138/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1904 - rmse: 39.1410 - val_loss: 46.2457 - val_rmse: 47.5134\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 43.42315\n",
      "Epoch 139/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2307 - rmse: 39.5093 - val_loss: 46.2556 - val_rmse: 47.5237\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 43.42315\n",
      "Epoch 140/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3947 - rmse: 39.5588 - val_loss: 46.2637 - val_rmse: 47.5322\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 43.42315\n",
      "Epoch 141/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3871 - rmse: 39.3601 - val_loss: 46.2502 - val_rmse: 47.5182\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 43.42315\n",
      "Epoch 142/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4462 - rmse: 39.5868 - val_loss: 46.2162 - val_rmse: 47.4826\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 43.42315\n",
      "Epoch 143/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1065 - rmse: 39.2116 - val_loss: 46.2085 - val_rmse: 47.4746\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 43.42315\n",
      "Epoch 144/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1151 - rmse: 39.1961 - val_loss: 46.2271 - val_rmse: 47.4939\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 43.42315\n",
      "Epoch 145/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6418 - rmse: 39.6662 - val_loss: 46.2200 - val_rmse: 47.4865\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 43.42315\n",
      "Epoch 146/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3238 - rmse: 39.1827 - val_loss: 46.2207 - val_rmse: 47.4871\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 43.42315\n",
      "Epoch 147/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3825 - rmse: 39.5411 - val_loss: 46.2186 - val_rmse: 47.4847\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 43.42315\n",
      "Epoch 148/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.3667 - rmse: 39.5864 - val_loss: 46.2112 - val_rmse: 47.4771\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 43.42315\n",
      "Epoch 149/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.7751 - rmse: 39.8494 - val_loss: 46.1920 - val_rmse: 47.4569\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 43.42315\n",
      "Epoch 150/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.2304 - rmse: 39.3333 - val_loss: 46.1814 - val_rmse: 47.4459\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 43.42315\n",
      "Epoch 151/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1797 - rmse: 39.3652 - val_loss: 46.2037 - val_rmse: 47.4693\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 43.42315\n",
      "Epoch 152/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.6460 - rmse: 39.5870 - val_loss: 46.2286 - val_rmse: 47.4951\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 43.42315\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 153/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.5581 - rmse: 39.6315 - val_loss: 46.2288 - val_rmse: 47.4953\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 43.42315\n",
      "Epoch 154/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3947 - rmse: 39.3926 - val_loss: 46.2278 - val_rmse: 47.4944\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 43.42315\n",
      "Epoch 155/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.1158 - rmse: 39.0938 - val_loss: 46.2268 - val_rmse: 47.4934\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 43.42315\n",
      "Epoch 156/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4375 - rmse: 39.4132 - val_loss: 46.2265 - val_rmse: 47.4931\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 43.42315\n",
      "Epoch 157/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.5673 - rmse: 39.3133 - val_loss: 46.2257 - val_rmse: 47.4921\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 43.42315\n",
      "Epoch 158/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.3225 - rmse: 39.5254 - val_loss: 46.2272 - val_rmse: 47.4938\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 43.42315\n",
      "Epoch 159/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.1304 - rmse: 39.3416 - val_loss: 46.2276 - val_rmse: 47.4942\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 43.42315\n",
      "Epoch 160/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4755 - rmse: 39.4136 - val_loss: 46.2265 - val_rmse: 47.4930\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 43.42315\n",
      "Epoch 161/500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 38.4206 - rmse: 39.6589 - val_loss: 46.2272 - val_rmse: 47.4937\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 43.42315\n",
      "Epoch 162/500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 38.3876 - rmse: 39.4775 - val_loss: 46.2281 - val_rmse: 47.4947\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 43.42315\n",
      "Epoch 00162: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    train,\n",
    "    epochs=500,\n",
    "    batch_size=4,\n",
    "    validation_split=0.25,\n",
    "    callbacks= callbacks\n",
    "    # validation_data=(x_test_scaled, x_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8684281f-e4f0-4172-9120-75a6256db6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8165f154-a30f-45e0-98f8-277c2381bf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "42ef0b27-bf30-4992-9c21-9471b9e9fb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 100), dtype=int64, numpy=\n",
       "array([[ 18,  18,  58, ...,  21,  21,   7],\n",
       "       [ 18,  58,  21, ...,  21,   7,  66],\n",
       "       [ 58,  21,  21, ...,   7,  66,   8],\n",
       "       ...,\n",
       "       [ 13,  18,   7, ...,  35,   7, 129],\n",
       "       [ 18,   7,  47, ...,   7, 129, 127],\n",
       "       [  7,  47,  16, ..., 129, 127, 126]], dtype=int64)>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_test = pd.Series(test_df.to_numpy().flatten()).apply(lambda x: ' '.join(x)).values.reshape(-1, 1).tolist()\n",
    "test = vectorize_layer(s_test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "027b1d52-c924-4135-aac8-fba7a17aedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(model, x_train):\n",
    "    reconstructions = model.predict(x_train)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tf.keras.losses.mae(reconstructions, x_train)\n",
    "    # threshold for anomaly scores\n",
    "    threshold = np.mean(reconstruction_errors.numpy()) + np.std(reconstruction_errors.numpy())\n",
    "    return threshold\n",
    "\n",
    "def get_predictions(model, x_test, threshold):\n",
    "    predictions = model.predict(x_test)\n",
    "    # provides losses of individual instances\n",
    "    errors = tf.keras.losses.mae(predictions, x_test)\n",
    "    # 0 = anomaly, 1 = normal\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "232031c6-532f-4a5f-bded-9d027b58e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 16.59912089482906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.183"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = find_threshold(model, test)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "predictions = get_predictions(model, test, threshold)\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e2d5b08f-fc6f-439f-8c8e-61abff1a3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.183\n",
      "Precision: 0.09854423292273236\n",
      "Recall: 0.88\n",
      "F1 Score: 0.17724068479355487\n",
      "Final Grade: 55.25573770491803/70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55.25573770491803"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef199bc3-0747-4bd6-a3e0-af6d67eb0c21",
   "metadata": {},
   "source": [
    "## Sklearn AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e133dfaa-9347-4f32-a004-f90361d4f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils as skl_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, learning_rate=0.02, epochs=20):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self._model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    def fit(self, df_x, df_y=None):\n",
    "        tagged_x = [TaggedDocument(str(row['Plot']).split(), [index]) for index, row in df_x.iterrows()]\n",
    "        model = Doc2Vec(documents=tagged_x, vector_size=self.vector_size, workers=self.workers)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            model.train(skl_utils.shuffle([x for x in tqdm(tagged_x)]), total_examples=len(tagged_x), epochs=1)\n",
    "            model.alpha -= self.learning_rate\n",
    "            model.min_alpha = model.alpha\n",
    "\n",
    "        self._model = model\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_x):\n",
    "        return np.asmatrix(np.array([self._model.infer_vector(str(row['Plot']).split())\n",
    "                                     for index, row in df_x.iterrows()]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bae1c85-2774-4b28-a3f8-4a021376241b",
   "metadata": {},
   "source": [
    "doc2vec_tr = Doc2VecTransformer(vector_size=300)\n",
    "doc2vec_tr.fit(train_df)\n",
    "doc2vec_vectors = doc2vec_tr.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "735b913e-fda2-4de4-8fab-8aa3a71e0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_vectors = embedding_model.wv[train_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3147d710-c01b-410d-bb65-af301dae6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "auto_encoder = MLPRegressor(hidden_layer_sizes=(\n",
    "                                                 64,\n",
    "                                                 16, \n",
    "                                                 64,\n",
    "                                               ))\n",
    "auto_encoder.fit(doc2vec_vectors, doc2vec_vectors)\n",
    "predicted_vectors = auto_encoder.predict(doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "68a05c4b-1ef9-44d8-aa23-6faf5e1c8949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.034977812176536"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_encoder.score(predicted_vectors, doc2vec_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0ad21178-241c-44b4-9e00-bfda85f97bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(auto_encoder.loss_curve_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18b255-bedf-4992-9078-b5dcbe2739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "version = input()\n",
    "\n",
    "dir_path = f'submissions\\\\{version}'\n",
    "notebook_name = f'318443595_318949443_{version}.ipynb'\n",
    "if not os.path.exists(dir_path):\n",
    "    os.system(f'mkdir {dir_path}')\n",
    "\n",
    "shutil.copyfile('Untitled.ipynb', f'{dir_path}\\\\{notebook_name}')\n",
    "to_fill_df = pd.read_csv('challengeToFillOriginal.csv')\n",
    "to_fill_df.iloc[LABELED_USERS: ,TRAIN_SEG+1:] = np.array(preds, dtype=int)\n",
    "for col in to_fill_df.columns[1:]:\n",
    "    to_fill_df[col] = to_fill_df[col].astype(int)\n",
    "to_fill_df.to_csv(f'submissions/{version}/318443595_318949443_{version}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fe775-1648-4b3c-8708-79abeb866f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-dl] *",
   "language": "python",
   "name": "conda-env-.conda-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
